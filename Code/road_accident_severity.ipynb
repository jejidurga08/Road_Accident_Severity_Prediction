{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "655aba71-7521-41d7-8a25-d1587052988a",
   "metadata": {},
   "source": [
    "# Road Accident Severity Prediction using Graph Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5467107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy==1.23.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fe6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "pip install torch_geometric\n",
    "pip install xgboost==0.90\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e26e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_geometric\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print('XGBoost version:', xgb.__version__)\n",
    "print('CUDA version:', torch.version.cuda)\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('PyG version:', torch_geometric.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as osp\n",
    "import networkx as nx\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.io import read_npz\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import reset, uniform, zeros\n",
    "from torch_geometric.typing import OptTensor, OptPairTensor, Adj, Size\n",
    "from torch_geometric.data import Data, DataLoader, InMemoryDataset, download_url\n",
    "from torch_geometric.data.data import DataEdgeAttr\n",
    "\n",
    "from pylab import cm\n",
    "from matplotlib import colors\n",
    "from IPython.display import clear_output\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from typing import Union, Tuple, Callable, Optional\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, roc_auc_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "np.random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea71f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available, otherwise use CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CUDA is not available, using CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff4ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\", \"Colorado\": \"CO\", \"Connecticut\": \"CT\", \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\", \"Georgia\": \"GA\", \"Hawaii\": \"HI\", \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\", \"Indiana\": \"IN\", \"Iowa\": \"IA\", \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\", \"Louisiana\": \"LA\", \"Maine\": \"ME\", \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\", \"Michigan\": \"MI\", \"Minnesota\": \"MN\", \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\", \"Montana\": \"MT\", \"Nebraska\": \"NE\", \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\", \"New Jersey\": \"NJ\", \"New Mexico\": \"NM\", \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\", \"North Dakota\": \"ND\", \"Ohio\": \"OH\", \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\", \"Pennsylvania\": \"PA\", \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\", \"Tennessee\": \"TN\", \"Texas\": \"TX\", \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\", \"Virginia\": \"VA\", \"Washington\": \"WA\", \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\", \"Wyoming\": \"WY\", \"District of Columbia\": \"DC\", \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\", \"Northern Mariana Islands\": \"MP\", \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\", \"U.S. Virgin Islands\": \"VI\",\n",
    "}\n",
    "\n",
    "us_abbrev_to_state = dict(map(reversed, us_state_to_abbrev.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad294d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npz(path):\n",
    "    with np.load(path, allow_pickle=True) as f:\n",
    "        return parse_npz(f)\n",
    "\n",
    "\n",
    "def parse_npz(f):\n",
    "    \"Set up severity prediction task here: use severity_labels as labels\"\n",
    "    crash_time = f['crash_time']\n",
    "    x = torch.from_numpy(f['x']).to(torch.float)\n",
    "    coords = torch.from_numpy(f['coordinates']).to(torch.float)\n",
    "    edge_attr = torch.from_numpy(f['edge_attr']).to(torch.float)\n",
    "    cnt_labels = torch.from_numpy(f['cnt_labels']).to(torch.long)\n",
    "    occur_labels = torch.from_numpy(f['occur_labels']).to(torch.long)\n",
    "    edge_attr_dir = torch.from_numpy(f['edge_attr_dir']).to(torch.float)\n",
    "    edge_attr_ang = torch.from_numpy(f['edge_attr_ang']).to(torch.float)\n",
    "    severity_labels = torch.from_numpy(f['severity_8labels']).to(torch.long)\n",
    "    edge_index = torch.from_numpy(f['edge_index']).to(torch.long).t().contiguous()\n",
    "    return Data(x=x, y=severity_labels, occur_labels=occur_labels, edge_index=edge_index,\n",
    "                edge_attr=edge_attr, edge_attr_dir=edge_attr_dir, edge_attr_ang=edge_attr_ang,\n",
    "                coords=coords, cnt_labels=cnt_labels, crash_time=crash_time)\n",
    "\n",
    "\n",
    "def train_test_split_stratify(dataset, train_ratio, val_ratio, class_num):\n",
    "    labels = dataset[0].y\n",
    "    train_mask = torch.zeros(size=labels.shape, dtype=bool)\n",
    "    val_mask = torch.zeros(size=labels.shape, dtype=bool)\n",
    "    test_mask = torch.zeros(size=labels.shape, dtype=bool)\n",
    "    for i in range(class_num):\n",
    "        stratify_idx = np.argwhere(labels.numpy() == i).flatten()\n",
    "        np.random.shuffle(stratify_idx)\n",
    "        split1 = int(len(stratify_idx) * train_ratio)\n",
    "        split2 = split1 + int(len(stratify_idx) * val_ratio)\n",
    "        train_mask[stratify_idx[:split1]] = True\n",
    "        val_mask[stratify_idx[split1:split2]] = True\n",
    "        test_mask[stratify_idx[split2:]] = True\n",
    "\n",
    "    highest = pd.DataFrame(labels).value_counts().head().iloc[0]\n",
    "    # print(\"Null Accuracy:\", highest / (len(labels)))\n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRAVELDataset(InMemoryDataset):\n",
    "    r\"\"\"The Traffic Accident Prediction (TAP) dataset introduced in the\n",
    "    `\"TAP: A Comprehensive Data Repository for Traffic Accident Prediction in Road Networks\"\n",
    "    <https://arxiv.org/pdf/2304.08640>`_ paper.\n",
    "    Nodes represent intersections and edges are roads.\n",
    "    Node and edge features represent embeddings of geospatial features.\n",
    "    The task is to predict the occurrence and severity of accidents on roadways.\n",
    "    For further information, please refer to the TAP repository.\n",
    "    `TAP\n",
    "    <https://github.com/baixianghuang/travel>`_\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        name (string): The name of the dataset.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    url = 'https://github.com/baixianghuang/travel/raw/main/TAP-city/{}.npz'\n",
    "    # url = 'https://github.com/baixianghuang/travel/raw/main/TAP-state/{}.npz'\n",
    "\n",
    "    def __init__(self, root: str, name: str,\n",
    "                 transform: Optional[Callable] = None,\n",
    "                 pre_transform: Optional[Callable] = None):\n",
    "        self.name = name.lower()\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        #self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0], weights_only=False)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'raw')\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.root, self.name, 'processed')\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> str:\n",
    "        return f'{self.name}.npz'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        download_url(self.url.format(self.name), self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        data = read_npz(self.raw_paths[0])\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.serialization.add_safe_globals([DataEdgeAttr])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.name.capitalize()}Full()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f51619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRAVELConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int or tuple): Size of each input sample, or :obj:`-1` to\n",
    "            derive the size from the first input(s) to the forward method.\n",
    "            A tuple corresponds to the sizes of source and target\n",
    "            dimensionalities.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        nn (torch.nn.Module): A neural network that\n",
    "            maps feature data of shape :obj:`[-1,\n",
    "            num_node_features + num_edge_features]` to shape\n",
    "            :obj:`[-1, new_dimension]`, *e.g.*, defined by\n",
    "            :class:`torch.nn.Sequential`.\n",
    "        aggr (string, optional): The aggregation scheme to use\n",
    "            (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"max\"`).\n",
    "            (default: :obj:`\"add\"`)\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add the transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: Union[int, Tuple[int, int]],\n",
    "                 out_channels: int, nn: Callable, aggr: str = 'add',\n",
    "                 root_weight: bool = True, bias: bool = True, **kwargs):\n",
    "        super(TRAVELConv, self).__init__(aggr=aggr, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.nn = nn\n",
    "        self.aggr = aggr\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.in_channels_l = in_channels[0]\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = Parameter(torch.Tensor(in_channels[1], out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.nn)\n",
    "        if self.root is not None:\n",
    "            uniform(self.root.size(0), self.root)\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if x_r is not None and self.root is not None:\n",
    "            out += torch.matmul(x_r, self.root)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_i: Tensor, x_j: Tensor, edge_attr: Tensor) -> Tensor:\n",
    "        inputs = torch.cat([x_j, edge_attr], dim=1)\n",
    "        return self.nn(inputs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, aggr=\"{}\", nn={})'.format(self.__class__.__name__,\n",
    "                                                     self.in_channels,\n",
    "                                                     self.out_channels,\n",
    "                                                     self.aggr, self.nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    logits, measures = model().detach(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        mea = f1_score(data.y[mask].cpu(), pred.cpu(), average='weighted',zero_division=0)\n",
    "        #test_jaccard = jaccard_score(data.y[mask].cpu(), pred.cpu(), average='weighted', zero_division=0)  \n",
    "        measures.append(mea)\n",
    "    label_pred = logits.max(1)[1]\n",
    "\n",
    "    mask = data.test_mask\n",
    "    #scores = logits[mask][:, 1]\n",
    "    #scores = logits[mask] \n",
    "    scores = torch.exp(logits[mask])\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    test_y = data.y[mask]\n",
    "\n",
    "    test_acc = pred.eq(test_y).sum().item() / mask.sum().item()\n",
    "    #return measures, label_pred, test_acc\n",
    "    # Calculate recall and precision for deep learning model\n",
    "    unique_predicted_labels = set(pred.cpu().numpy())\n",
    "    unique_true_labels = set(data.y[mask].cpu().numpy())\n",
    "    all_labels = sorted(list(unique_predicted_labels.union(unique_true_labels)))\n",
    "\n",
    "    test_recall = recall_score(data.y[mask].cpu(), pred.cpu(), labels=all_labels, average='weighted', zero_division=0)\n",
    "    test_precision = precision_score(data.y[mask].cpu(), pred.cpu(), labels=all_labels, average='weighted', zero_division=0)\n",
    "    test_jaccard = jaccard_score(data.y[mask].cpu(), pred.cpu(), average='weighted', zero_division=0)  \n",
    "\n",
    "    #return measures, label_pred, test_acc, test_recall, test_precision\n",
    "    # Calculate specificity, ROC-AUC, and MCC\n",
    "    cm = confusion_matrix(data.y[mask].cpu(), pred.cpu(), labels=all_labels)\n",
    "    tn = cm[0, 0]  # True negatives\n",
    "    fp = cm[0, 1]  # False positives\n",
    "    fn = cm[1, 0]  # False negatives\n",
    "    tp = cm[1, 1]  # True positives\n",
    "\n",
    "    test_specificity = tn / (tn + fp) if (tn + fp) != 0 else 0  # Handle zero division\n",
    "    #test_roc_auc = roc_auc_score(data.y[mask].cpu(), scores[:, 1].cpu())  # Use probabilities for class 1\n",
    "    try:\n",
    "        test_roc_auc = roc_auc_score(data.y[mask].cpu(), scores.cpu(), multi_class='ovr', average='weighted') # Multi-class ROC-AUC calculation\n",
    "    except ValueError:\n",
    "        test_roc_auc = 0  # Handle cases with only one class\n",
    "\n",
    "    test_mcc = matthews_corrcoef(data.y[mask].cpu(), pred.cpu())\n",
    "\n",
    "    return measures, label_pred, test_acc, test_recall, test_precision, test_specificity, test_mcc, test_jaccard \n",
    "\n",
    "def train_loop(model, data, optimizer, num_epochs, model_name='', city_name=''):\n",
    "    #epochs, train_measures, valid_measures, test_measures, test_accs = [], [], [], [], []\n",
    "    epochs, train_measures, valid_measures, test_measures, test_accs = [0], [0], [0], [0], [0]  # Initialize with 0\n",
    "    test_recalls, test_precisions, test_specificities, test_roc_aucs, test_mccs = [0], [0], [0], [0], [0] \n",
    "    coords = data.coords.cpu().numpy()\n",
    "    gdf_pred = pd.DataFrame({'x': coords[:, 0], 'y': coords[:, 1]})\n",
    "    test_jaccards = [0]  \n",
    "    for epoch in range(num_epochs):\n",
    "        train(model, data, optimizer)\n",
    "        log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "        #measures, label_pred, test_acc = test(model, data)\n",
    "        #measures, label_pred, test_acc, test_recall, test_precision = test(model, data)\n",
    "        measures, label_pred, test_acc, test_recall, test_precision, test_specificity, test_mcc ,test_jaccard= test(model, data)\n",
    "        train_mea, valid_mea, test_mea = measures\n",
    "        epochs.append(epoch)\n",
    "        train_measures.append(train_mea)\n",
    "        valid_measures.append(valid_mea)\n",
    "        test_measures.append(test_mea)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "\n",
    "        # Append new metrics to their respective lists\n",
    "        test_recalls.append(test_recall)\n",
    "        test_precisions.append(test_precision)\n",
    "        test_specificities.append(test_specificity)\n",
    "        #test_roc_aucs.append(test_roc_auc)\n",
    "        test_mccs.append(test_mcc)\n",
    "        test_jaccards.append(test_jaccard)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "          clear_output(True)\n",
    "          fig, (ax1, ax) = plt.subplots(1, 2, figsize=(30, 12))\n",
    "          gdf_pred['label'] = label_pred.cpu().numpy()\n",
    "          for i in range(class_num):\n",
    "            G = nx.MultiGraph()\n",
    "            G.add_nodes_from(gdf_pred[gdf_pred['label'] == i].index)\n",
    "            sub1 = nx.draw(G, pos=pos_dict, ax=ax1, node_color=colors.rgb2hex(cmap(i)), node_size=10, label=i)\n",
    "\n",
    "        # Set title and axis labels for the severity map (ax1)\n",
    "          ax1.set_title(f\"{model_name}-based Severity Map\")\n",
    "          ax1.set_xlabel(f\"{model_name}-based Severity Map\")\n",
    "          ax1.set_ylabel(\"Severity\")\n",
    "\n",
    "        # Plotting performance graph (ax)\n",
    "          ax.text(1, 1, log.format(epoch, train_measures[-1], valid_measures[-1], test_measures[-1]), fontsize=18)\n",
    "          ax.plot(epochs, train_measures, \"r\", label=\"Train\") # Add label for train\n",
    "          ax.plot(epochs, valid_measures, \"g\", label=\"Valid\") # Add label for valid\n",
    "          ax.plot(epochs, test_measures, \"b\", label=\"Test\")   # Add label for test\n",
    "          ax.set_ylim([0, 1])\n",
    "\n",
    "        # Set title and axis labels for the performance graph (ax)\n",
    "\n",
    "          ax.set_xlabel(\"Epoch\")             # Label for x-axis\n",
    "          ax.set_ylabel(\"Score\")            # Label for y-axis\n",
    "          ax.legend()                        # Show legend\n",
    "\n",
    "          norm = colors.BoundaryNorm(boundaries=range(8), ncolors=8)\n",
    "          plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax1, orientation='vertical', label='Severity')\n",
    "\n",
    "          plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    select_idx = np.argmax(valid_measures[num_epochs//2:]) + num_epochs//2\n",
    "    final_test_mea = np.array(test_measures)[select_idx]\n",
    "    final_test_acc = np.array(test_accs)[select_idx]\n",
    "    print('F measure {:.5f} | Test Accuracy {:.5f} | Recall {:.5f} | Precision {:.5f} | Specificity {:.5f} | MCC {:.5f} | Jaccard Index {:.5f}'.format(  # Remove ROC-AUC from print\n",
    "        final_test_mea, final_test_acc, test_recall, test_precision, test_specificity, test_mcc, test_jaccard))\n",
    "    return (round(final_test_mea*100, 2), round(final_test_acc*100, 2), round(test_recall*100,2), round(test_precision*100,2),\n",
    "           round(test_specificity*100, 2), round(test_mcc*100, 2), round(test_jaccard*100, 2)) # Remove ROC-AUC from return values\n",
    "    '''print('F measure {:.5f} | Test Accuracy {:.5f} | Recall {:.5f} | Precision {:.5f} | Specificity {:.5f} | ROC-AUC {:.5f} | MCC {:.5f}'.format(\n",
    "        final_test_mea, final_test_acc, test_recall, test_precision, test_specificity, test_mcc,test_jaccard))\n",
    "    return (round(final_test_mea*100, 2), round(final_test_acc*100, 2), round(test_recall*100,2), round(test_precision*100,2),\n",
    "           round(test_specificity*100, 2), round(test_mcc*100, 2), round(test_jaccard*100, 2))'''\n",
    "    \n",
    "    '''print('F measure {:.5f} | Test Accuracy {:.5f} | Recall {:.5f} | Precision {:.5f}'.format(\n",
    "        final_test_mea, final_test_acc, test_recall, test_precision)) # Print recall and precision\n",
    "    return (round(final_test_mea*100, 2), round(final_test_acc*100, 2), round(test_recall*100,2), round(test_precision*100,2))  # Return recall and precision'''\n",
    "    #print('F measure {:.5f} | Test Accuracy {:.5f}'.format(final_test_mea, final_test_acc))\n",
    "    #return (round(final_test_mea*100, 2), round(final_test_acc*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def draw_with_labels(df_nodes, model_name='test', city_name='City Name'):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))  # Create a figure and an axes object\n",
    "\n",
    "    for i in range(class_num):\n",
    "        G = nx.MultiGraph()\n",
    "        G.add_nodes_from(df_nodes[df_nodes['label'] == i].index)\n",
    "        nx.draw(G, pos=pos_dict, node_color=colors.rgb2hex(cmap(i)), node_size=3, label=i, ax=ax)  # Draw on the axes\n",
    "\n",
    "    norm = colors.BoundaryNorm(boundaries=range(8), ncolors=8)\n",
    "    plt.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax, orientation='vertical', label='Severity')  # Specify the axes\n",
    "\n",
    "\n",
    "    ax.set_title(f\"{model_name} Based Severity Map\")  # Include city name in title\n",
    "    ax.set_xlabel(f\"{model_name} Based Severity Map\")\n",
    "    ax.set_ylabel(\"Severity\")\n",
    "    plt.show()\n",
    "\n",
    "d=10\n",
    "p=0.5\n",
    "all_res = []\n",
    "class_num = 8\n",
    "num_epochs = 100\n",
    "file_path = 'exp-severity/'\n",
    "cmap = cm.get_cmap('cool', class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_dim=d):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(dataset.num_features, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x = F.relu(self.fc1(data.x))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=d):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(dataset.num_features, hidden_dim)\n",
    "        self.conv2 = pyg_nn.GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = pyg_nn.SAGEConv(dataset.num_features, dim)\n",
    "        self.conv2 = pyg_nn.SAGEConv(dim, dim*2, normalize=True)\n",
    "        self.fc1 = nn.Linear(dim*2, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.conv1 = pyg_nn.TransformerConv(dataset.num_features, dim, edge_dim=edge_attr_all.shape[1])\n",
    "        self.conv2 = pyg_nn.TransformerConv(dim, dim, edge_dim=edge_attr_all.shape[1])\n",
    "        self.fc1 = nn.Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, edge_attr_all\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr))\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "class TRAVELNet(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(TRAVELNet, self).__init__()\n",
    "        convdim = 8\n",
    "        self.node_encoder = nn.Sequential(nn.Linear(data.x.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "        self.edge_encoder_dir = nn.Sequential(nn.Linear(data.component_dir.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "        self.edge_encoder_ang = nn.Sequential(nn.Linear(data.component_ang.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "        nn1 = nn.Sequential(nn.Linear(dim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, convdim))\n",
    "        self.conv1 = TRAVELConv(dim, convdim, nn1)\n",
    "        nn2 = nn.Sequential(nn.Linear(2*convdim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dataset.num_classes))\n",
    "        self.conv2 = TRAVELConv(2*convdim, dataset.num_classes, nn2)\n",
    "        self.bn1 = nn.BatchNorm1d(convdim*2)\n",
    "        nn1_2 = nn.Sequential(nn.Linear(dim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, convdim))\n",
    "        self.conv1_2 = TRAVELConv(dim, convdim, nn1_2)\n",
    "        nn2_2 = nn.Sequential(nn.Linear(2*convdim + dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dataset.num_classes))\n",
    "        self.conv2_2 = TRAVELConv(2*convdim, dataset.num_classes, nn2_2)\n",
    "        self.bn2 = nn.BatchNorm1d(dataset.num_classes*2)\n",
    "        self.fc = nn.Linear(dataset.num_classes*2, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = self.node_encoder(data.x), data.edge_index\n",
    "        edge_attr_dir, edge_attr_ang = self.edge_encoder_dir(data.component_dir), self.edge_encoder_ang(data.component_ang)\n",
    "        x1 = F.relu(self.conv1(x, edge_index, edge_attr_dir))\n",
    "        x2 = F.relu(self.conv1_2(x, edge_index, edge_attr_ang))\n",
    "        x = torch.cat((x1, x2), axis=1)\n",
    "        x = self.bn1(x)\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x1 = F.relu(self.conv2(x, edge_index, edge_attr_dir))\n",
    "        x2 = F.relu(self.conv2_2(x, edge_index, edge_attr_ang))\n",
    "        x = torch.cat((x1, x2), axis=1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class TRAVELAng(torch.nn.Module):\n",
    "    def __init__(self, dim=d):\n",
    "        super(TRAVELAng, self).__init__()\n",
    "        self.node_encoder = nn.Sequential(nn.Linear(data.x.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "        self.edge_encoder_ang = nn.Sequential(nn.Linear(data.component_ang.size(-1), dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "        nn1 = nn.Sequential(nn.Linear(dim+dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "        self.conv1 = TRAVELConv(dim, dim, nn1)\n",
    "        self.bn1 = nn.BatchNorm1d(dim)\n",
    "        nn2 = nn.Sequential(nn.Linear(dim+dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim), nn.LeakyReLU(), nn.Linear(dim, dim))\n",
    "        self.conv2 = TRAVELConv(dim, dim, nn2)\n",
    "        self.fc = nn.Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = self.node_encoder(data.x), data.edge_index\n",
    "        edge_attr_encoded = self.edge_encoder_ang(data.component_ang)\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr_encoded))\n",
    "        x = self.bn1(x)\n",
    "        x = F.dropout(x, p=p, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index, edge_attr_encoded))\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed54cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch_geometric.data.data\n",
    "\n",
    "# Add DataEdgeAttr to safe globals\n",
    "torch.serialization.add_safe_globals([torch_geometric.data.data.DataEdgeAttr])\n",
    "\n",
    "all_res = []\n",
    "for e in [('Los Angeles', 'California')]:\n",
    "    city_name, state_abbrev = e[0].lower().replace(\" \", \"_\"), us_state_to_abbrev[e[1]].lower()\n",
    "    city_format = e[0]+' ('+us_state_to_abbrev[e[1]]+')'\n",
    "    if os.path.exists(file_path+city_name+'_'+state_abbrev+'/processed'):\n",
    "        shutil.rmtree(file_path+city_name+'_'+state_abbrev+'/processed')\n",
    "    dataset = TRAVELDataset(file_path, city_name+'_'+state_abbrev)\n",
    "    data = dataset[0]\n",
    "    class_num = dataset.num_classes\n",
    "\n",
    "    # 60%, 20% and 20% for training, validation and test\n",
    "    data.train_mask, data.val_mask, data.test_mask = train_test_split_stratify(dataset, train_ratio=0.6, val_ratio=0.2, class_num=class_num)\n",
    "    sc = MinMaxScaler()\n",
    "    #data.x[data.train_mask] = torch.tensor(sc.fit_transform(data.x[data.train_mask]), dtype=torch.float)\n",
    "    #data.x[data.val_mask] = torch.tensor(sc.transform(data.x[data.val_mask]), dtype=torch.float)\n",
    "    #data.x[data.test_mask] = torch.tensor(sc.transform(data.x[data.test_mask]), dtype=torch.float)\n",
    "    data.x[data.train_mask] = torch.tensor(sc.fit_transform(data.x[data.train_mask]), dtype=torch.float32) # Change to float32\n",
    "    data.x[data.val_mask] = torch.tensor(sc.transform(data.x[data.val_mask]), dtype=torch.float32)      # Change to float32\n",
    "    data.x[data.test_mask] = torch.tensor(sc.transform(data.x[data.test_mask]), dtype=torch.float32)      # Change to float32\n",
    "    edge_attr_all = MinMaxScaler().fit_transform(data.edge_attr.cpu())\n",
    "    edge_attr_all = torch.tensor(edge_attr_all).float().to('cpu')\n",
    "\n",
    "    coords = data.coords.numpy()\n",
    "    gdf_pred = pd.DataFrame({'x': coords[:, 0], 'y': coords[:, 1], 'label': data.y.numpy()})\n",
    "    zip_iterator = zip(gdf_pred.index, gdf_pred[['x', 'y']].values)\n",
    "    pos_dict = dict(zip_iterator)\n",
    "    draw_with_labels(gdf_pred, 'Ground Truth')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = data.x[data.train_mask].cpu().numpy(), data.x[data.test_mask].cpu().numpy(), data.y[data.train_mask].cpu().numpy(), data.y[data.test_mask].cpu().numpy()\n",
    "    start_time = time.time()\n",
    "    xgb_clf = XGBClassifier(objective='multi:softmax', eval_metric='mlogloss')\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    y_pred = xgb_clf.predict(X_test)\n",
    "    test_acc, test_f1,test_recall,test_precision = accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted'),recall_score(y_test, y_pred, average='weighted',zero_division=0),precision_score(y_test, y_pred, average='weighted',zero_division=0)\n",
    "    #print('F measure {:.5f} | Test Accuracy {:.5f}'.format(test_f1, test_acc))\n",
    "    print('F measure {:.5f} | Test Accuracy {:.5f} | Recall {:.5f} | Precision {:.5f}'.format(test_f1, test_acc, test_recall, test_precision))\n",
    "    #res = (round(test_f1*100, 2), round(test_acc*100, 2))\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    res = (round(test_f1*100, 2), round(test_acc*100, 2), round(test_recall*100, 2), round(test_precision*100, 2), t)  # Include time in res\n",
    "    all_res.append((city_format,) + ('XGBoost',) + res)  # Append res directly\n",
    "    print(\"Execution time: %.4f seconds\" % t)\n",
    "    y_pred_all = xgb_clf.predict(data.x.cpu().numpy())\n",
    "    df_pred = pd.DataFrame({'x': coords[:, 0], 'y': coords[:, 1], 'label': y_pred_all})\n",
    "    #draw_with_labels(df_pred, 'XGBoost')\n",
    "    draw_with_labels(df_pred, 'XGBoost', city_format)\n",
    "\n",
    "    data = data.to('cpu')\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = MLP().to('cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    res = train_loop(model, data, optimizer, num_epochs, 'MLP')\n",
    "    t = round(time.time() - start_time, 2)\n",
    "    all_res.append((city_format,) + ('MLP',) + res + (t,))\n",
    "    print(\"Execution time: %.4f seconds\" % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = GCN().to('cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "res = train_loop(model, data, optimizer, num_epochs, 'GCN')\n",
    "t = round(time.time() - start_time, 2)\n",
    "all_res.append((city_format,) + ('GCN',) + res + (t,))\n",
    "print(\"Execution time: %.4f seconds\" % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = GraphSAGE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "data = data.to(device)\n",
    "res = train_loop(model, data, optimizer, num_epochs, 'GraphSAGE')\n",
    "t = round(time.time() - start_time, 2)\n",
    "all_res.append((city_format,) + ('GraphSAGE',) + res + (t,))\n",
    "print(\"Execution time: %.4f seconds\" % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc4db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = GraphTransformer().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "data = data.to(device)\n",
    "res = train_loop(model, data, optimizer, num_epochs, 'Transformer')\n",
    "t = round(time.time() - start_time, 2)\n",
    "all_res.append((city_format,) + ('Transformer',) + res + (t,))\n",
    "print(\"Execution time: %.4f seconds\" % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "component_dir = np.concatenate((data.edge_attr.cpu(), data.edge_attr_dir.cpu()), axis=1)\n",
    "component_ang = np.concatenate((data.edge_attr.cpu(), data.edge_attr_ang.cpu()), axis=1)\n",
    "component_dir = StandardScaler().fit_transform(component_dir)\n",
    "component_ang = StandardScaler().fit_transform(component_ang)\n",
    "data.component_dir = torch.tensor(component_dir).float().to(device)\n",
    "data.component_ang = torch.tensor(component_ang).float().to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "model = TRAVELNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.006, weight_decay=5e-4)\n",
    "data = data.to(device)\n",
    "res = train_loop(model, data, optimizer, num_epochs, 'TRAVEL')\n",
    "t = round(time.time() - start_time, 2)\n",
    "all_res.append((city_format,) + ('TRAVEL',) + res + (t,))\n",
    "print(\"Execution time: %.4f seconds\" % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = TRAVELAng().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.006, weight_decay=5e-4)\n",
    "data = data.to(device)\n",
    "res = train_loop(model, data, optimizer, num_epochs, 'TRAVEL-ang')\n",
    "t = round(time.time() - start_time, 2)\n",
    "all_res.append((city_format,) + ('TRAVEL-ang',) + res + (t,))\n",
    "print(\"Execution time: %.4f seconds\" % t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_res, columns=['City', 'Method', 'F1-Score', 'Accuracy','Recall','Precision', 'Specificity', 'MCC','Jaccard Index', 'Time'])\n",
    "print('# datasets:', df.shape[0] // len(df.Method.unique()))\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "v12_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
